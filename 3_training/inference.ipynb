{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Voice Inference — Generate from Fine-Tuned Models\n\nLoad your saved fine-tuned models and generate essays/Notes from any prompt.\nNo training here — just loading weights and writing.\n\n## Prerequisites\n\n1. **Set runtime to GPU** (Runtime → Change runtime type → T4 GPU).\n2. **Have saved models on Google Drive** from the training notebooks.\n\n## Cell Map\n\n| Cell | What it does |\n|------|--------------|\n| 1 | Install dependencies |\n| 2 | Mount Drive + config |\n| 3 | Load fine-tuned Llama (LoRA adapter) |\n| 4 | Generate from Llama |\n| 5 | Load fine-tuned GPT-2-XL |\n| 6 | Generate from GPT-2 |\n| 7 | Side-by-side comparison |\n| 8 | Gradio web UI (shareable link) |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# === Cell 1: Install dependencies ===\n!pip install -q transformers peft bitsandbytes accelerate gradio"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 2: Mount Drive + config ===\n",
    "\n",
    "import torch\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "DRIVE_BASE = \"/content/drive/MyDrive/voice-ft\"\n",
    "\n",
    "# Paths to saved models — update these if you saved to a different location\n",
    "LLAMA_ADAPTER_PATH = f\"{DRIVE_BASE}/adapters/llama-voice-v1\"\n",
    "GPT2_MODEL_PATH = f\"{DRIVE_BASE}/models/gpt2-voice-v1\"\n",
    "\n",
    "# Generation hyperparameters — same as training notebooks\n",
    "GEN_KWARGS = dict(\n",
    "    temperature=0.8,\n",
    "    top_p=0.9,\n",
    "    top_k=50,\n",
    "    repetition_penalty=1.1,\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "print(f\"Llama adapter: {LLAMA_ADAPTER_PATH}\")\n",
    "print(f\"GPT-2 model: {GPT2_MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 3: Load fine-tuned Llama (base model + LoRA adapter) ===\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "from huggingface_hub import login\n",
    "from google.colab import userdata\n",
    "\n",
    "login(token=userdata.get(\"HF_TOKEN\"))\n",
    "\n",
    "MODEL_ID = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(\"Loading base Llama in 4-bit...\")\n",
    "llama_base = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "print(\"Applying LoRA adapter...\")\n",
    "llama_model = PeftModel.from_pretrained(llama_base, LLAMA_ADAPTER_PATH)\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(LLAMA_ADAPTER_PATH)\n",
    "\n",
    "print(f\"Llama loaded on: {llama_model.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 4: Generate from Llama ===\n",
    "# Change the prompt and max_new_tokens to whatever you want.\n",
    "# Short Note/Tweet: max_new_tokens=256\n",
    "# Full essay: max_new_tokens=1024 or higher\n",
    "\n",
    "prompt = \"Write an essay about why dark humor saved my life\"  # <-- EDIT THIS\n",
    "max_new_tokens = 1024  # <-- shorter for Notes, longer for essays\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "input_text = llama_tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "inputs = llama_tokenizer(input_text, return_tensors=\"pt\").to(llama_model.device)\n",
    "input_len = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = llama_model.generate(**inputs, **GEN_KWARGS, max_new_tokens=max_new_tokens)\n",
    "\n",
    "llama_output = llama_tokenizer.decode(output[0][input_len:], skip_special_tokens=True)\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "print(f\"{'─'*60}\\n\")\n",
    "print(llama_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 5: Load fine-tuned GPT-2-XL ===\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "print(\"Loading fine-tuned GPT-2-XL...\")\n",
    "gpt2_model = AutoModelForCausalLM.from_pretrained(GPT2_MODEL_PATH, torch_dtype=torch.float16)\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(GPT2_MODEL_PATH)\n",
    "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gpt2_model = gpt2_model.to(device)\n",
    "\n",
    "print(f\"GPT-2-XL loaded on: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 6: Generate from GPT-2 ===\n",
    "# Uses completion-style format (prompt + separator, then model continues).\n",
    "# GPT-2 maxes out at ~512 new tokens within its 1,024 context window.\n",
    "\n",
    "prompt = \"Write an essay about why dark humor saved my life\"  # <-- EDIT THIS\n",
    "max_new_tokens = 512\n",
    "\n",
    "input_text = f\"{prompt}\\n\\n---\\n\\n\"\n",
    "inputs = gpt2_tokenizer(input_text, return_tensors=\"pt\").to(gpt2_model.device)\n",
    "input_len = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = gpt2_model.generate(\n",
    "        **inputs, **GEN_KWARGS,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        pad_token_id=gpt2_tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "gpt2_output = gpt2_tokenizer.decode(output[0][input_len:], skip_special_tokens=True)\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "print(f\"{'─'*60}\\n\")\n",
    "print(gpt2_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 7: Side-by-side comparison ===\n",
    "# Run this after generating from both models with the same prompt.\n",
    "\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"\\n{'#'*70}\")\n",
    "print(f\"  LLAMA (fine-tuned, LoRA)\")\n",
    "print(f\"{'#'*70}\\n\")\n",
    "print(llama_output)\n",
    "print(f\"\\n{'#'*70}\")\n",
    "print(f\"  GPT-2-XL (fine-tuned, full)\")\n",
    "print(f\"{'#'*70}\\n\")\n",
    "print(gpt2_output)"
   ]
  },
  {
   "cell_type": "code",
   "source": "# === Cell 8: Gradio web UI ===\n# Launches a web app with a shareable public link (expires after 72 hours).\n# Anyone with the link can type prompts and get outputs — no setup needed.\n# Uses whichever models are loaded above. If both are loaded, shows both.\n\nimport gradio as gr\n\ndef generate_llama_fn(prompt, max_tokens):\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n    input_text = llama_tokenizer.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=True\n    )\n    inputs = llama_tokenizer(input_text, return_tensors=\"pt\").to(llama_model.device)\n    input_len = inputs[\"input_ids\"].shape[1]\n    with torch.no_grad():\n        output = llama_model.generate(**inputs, **GEN_KWARGS, max_new_tokens=int(max_tokens))\n    return llama_tokenizer.decode(output[0][input_len:], skip_special_tokens=True)\n\ndef generate_gpt2_fn(prompt, max_tokens):\n    input_text = f\"{prompt}\\n\\n---\\n\\n\"\n    inputs = gpt2_tokenizer(input_text, return_tensors=\"pt\").to(gpt2_model.device)\n    input_len = inputs[\"input_ids\"].shape[1]\n    with torch.no_grad():\n        output = gpt2_model.generate(\n            **inputs, **GEN_KWARGS,\n            max_new_tokens=int(max_tokens),\n            pad_token_id=gpt2_tokenizer.eos_token_id,\n        )\n    return gpt2_tokenizer.decode(output[0][input_len:], skip_special_tokens=True)\n\n# Build tabs based on which models are loaded\ntabs = []\n\nif \"llama_model\" in dir():\n    tabs.append(gr.Interface(\n        fn=generate_llama_fn,\n        inputs=[\n            gr.Textbox(label=\"Prompt\", lines=3, placeholder=\"Write an essay about...\"),\n            gr.Slider(128, 2048, value=1024, step=128, label=\"Max tokens\"),\n        ],\n        outputs=gr.Textbox(label=\"Output\", lines=25),\n        title=\"Llama 3.1 8B — Fine-Tuned (LoRA)\",\n    ))\n\nif \"gpt2_model\" in dir():\n    tabs.append(gr.Interface(\n        fn=generate_gpt2_fn,\n        inputs=[\n            gr.Textbox(label=\"Prompt\", lines=3, placeholder=\"Write an essay about...\"),\n            gr.Slider(128, 512, value=512, step=64, label=\"Max tokens\"),\n        ],\n        outputs=gr.Textbox(label=\"Output\", lines=25),\n        title=\"GPT-2-XL — Fine-Tuned (Full)\",\n    ))\n\nif tabs:\n    app = gr.TabbedInterface(tabs, tab_names=[\"Llama\", \"GPT-2\"][:len(tabs)])\n    app.launch(share=True)\nelse:\n    print(\"No models loaded. Run Cell 3 (Llama) and/or Cell 5 (GPT-2) first.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}