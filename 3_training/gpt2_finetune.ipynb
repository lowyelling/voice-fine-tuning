{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-2-XL Full Fine-Tuning — Voice\n",
    "\n",
    "Fine-tune GPT-2-XL (1.5B parameters) on essay pairs to capture writing voice.\n",
    "Full fine-tuning — every weight gets updated, no LoRA adapter.\n",
    "\n",
    "**Why GPT-2-XL?** It's pre-RLHF (released 2019, before alignment training existed),\n",
    "so it's a \"raw\" model. Comparing it against Llama (post-RLHF) answers the question:\n",
    "does alignment training help or hurt voice capture?\n",
    "\n",
    "**Why full fine-tuning instead of LoRA?** GPT-2-XL is small enough (1.5B params) to\n",
    "fine-tune fully on a T4 GPU. Full fine-tuning gives maximum capacity to learn voice,\n",
    "and trains in minutes. The tradeoff: higher overfitting risk on a small dataset.\n",
    "\n",
    "**Context window:** 1,024 tokens. This limits GPT-2 to short pieces (Notes, openings).\n",
    "The constraint is actually useful — it forces a clean 1:1 comparison with Llama on\n",
    "short pairs, isolating voice at the sentence level without essay-level structure.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. **Set runtime to GPU** (Runtime → Change runtime type → T4 GPU).\n",
    "2. That's it. No license, no token, no approval wait. GPT-2-XL is fully open."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# bitsandbytes is needed for the 8-bit Adam optimizer (saves ~9GB VRAM during training)\n!pip install -q transformers datasets bitsandbytes",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Upload training data from your local machine\n",
    "#\n",
    "# This opens a file picker. Select your JSONL files:\n",
    "#   - gpt2_train.jsonl\n",
    "#   - gpt2_val.jsonl\n",
    "#\n",
    "# These were generated locally by running:\n",
    "#   python3 2_scripts/format_jsonl.py\n",
    "#\n",
    "# The files live in Colab's temporary storage for this session.\n",
    "# They'll disappear when the runtime disconnects, but that's fine —\n",
    "# the trained model gets saved to Google Drive.\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "import os\n",
    "from google.colab import files\n",
    "\n",
    "print(\"Select your JSONL files (gpt2_train.jsonl and gpt2_val.jsonl):\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "DATA_DIR = \"/content/data\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "for filename in uploaded:\n",
    "    dest = os.path.join(DATA_DIR, filename)\n",
    "    os.rename(filename, dest)\n",
    "    print(f\"  {filename} → {dest}\")\n",
    "\n",
    "print(f\"\\nUploaded {len(uploaded)} file(s) to {DATA_DIR}/\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import torch\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForLanguageModeling,\n)\nfrom datasets import load_dataset\nfrom google.colab import drive\n\n# Mount Google Drive for saving the trained model\ndrive.mount(\"/content/drive\")\n\n# ---------------------------------------------------------------------------\n# Config\n# ---------------------------------------------------------------------------\n\n# GPT-2-XL: 1.5B parameters, 1,024 token context window.\n# No quantization needed for inference — it fits on a T4 as-is.\n# No LoRA — we fine-tune every weight directly.\n# No chat template — GPT-2 is a plain completion model. It sees:\n#   \"Write about X...\\n\\n---\\n\\nThe essay text...\"\n# and learns to predict what comes after the ---.\n#\n# Memory during training is tighter than you'd think:\n#   Model weights: ~6GB\n#   Optimizer states (Adam keeps 2 copies): ~12GB in fp32\n#   Gradients: ~6GB\n#   Total in fp32: ~24GB — exceeds T4's 16GB!\n#\n# Fix: fp16 halves the gradient/activation memory, and 8-bit Adam\n# (via bitsandbytes) shrinks optimizer states from 12GB to ~3GB.\n# Together they bring it under 16GB comfortably.\nMODEL_ID = \"gpt2-xl\"\n\n# Training hyperparameters\nLR = 5e-5          # Lower than Llama's 2e-4 because full fine-tuning updates\n                    # ALL weights, not just a small adapter. Too high = catastrophic\n                    # forgetting (the model forgets how to write English).\nEPOCHS = 3\nBATCH_SIZE = 1\nMAX_SEQ_LEN = 1024  # GPT-2's hard limit. Pairs longer than this get truncated.\n\n# Generation hyperparameters — identical across all 4 models for fair comparison\nGEN_KWARGS = dict(\n    temperature=0.8,\n    top_p=0.9,\n    top_k=50,\n    repetition_penalty=1.1,\n    max_new_tokens=512,   # Shorter than Llama (1024) because GPT-2's total window is 1024.\n                          # Prompt + output must fit in 1024 tokens total.\n    do_sample=True,\n)\n\nN_SAMPLES = 5\n\n# Paths\nDATA_DIR = \"/content/data\"\nDRIVE_BASE = \"/content/drive/MyDrive/voice-ft\"\nCHECKPOINT_DIR = f\"{DRIVE_BASE}/checkpoints/gpt2\"\n\nprint(f\"Model: {MODEL_ID}\")\nprint(f\"LR={LR}, epochs={EPOCHS}, batch={BATCH_SIZE}, max_seq_len={MAX_SEQ_LEN}\")\nprint(f\"Data dir: {DATA_DIR}\")\nprint(f\"Checkpoints: {CHECKPOINT_DIR}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Load GPT-2-XL\n",
    "#\n",
    "# No quantization, no special preparation. Just load and go.\n",
    "# Compare this to the Llama notebook's Cell 4 — that one needs:\n",
    "#   - HuggingFace login\n",
    "#   - BitsAndBytesConfig for 4-bit quantization\n",
    "#   - prepare_model_for_kbit_training()\n",
    "# GPT-2-XL skips all of that because it's small enough to fit as-is.\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "# GPT-2 has no pad token by default. Same fix as Llama: use eos_token.\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_ID)\n",
    "\n",
    "# Move to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  (100% trainable — full fine-tuning, no LoRA)\")\n",
    "print(f\"Model on: {device}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Canary prompts — same prompts as the Llama notebook.\n",
    "# Replace these with your actual canary prompts from\n",
    "# 4_experiments/canary-prompts.md\n",
    "#\n",
    "# Only A and B for GPT-2 (both short). Canary C is Llama-only because\n",
    "# it requires essay-length output that won't fit in 1,024 tokens.\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "canary_prompts = {\n",
    "    \"A\": \"Write a short essay about why writing transforms how you think. Thesis: writing is not a record of thought — it is the act of thinking itself.\",\n",
    "    \"B\": \"Write a short essay about what rock climbing teaches you about fear. Thesis: the hardest part is not the wall — it is trusting your hands after they've slipped.\",\n",
    "}\n",
    "\n",
    "\n",
    "def generate(model, tokenizer, prompt, n=N_SAMPLES):\n",
    "    \"\"\"Generate n samples from the model for a given prompt.\n",
    "\n",
    "    GPT-2 is a plain completion model — no chat template.\n",
    "    We feed the prompt directly and the model continues from there.\n",
    "\n",
    "    Compare to the Llama notebook's generate():\n",
    "      - Llama uses tokenizer.apply_chat_template() to wrap the prompt\n",
    "        with special [INST] tokens and role markers.\n",
    "      - GPT-2 just sees raw text. The prompt format matches the training\n",
    "        data: 'prompt text\\n\\n---\\n\\n' and the model continues after that.\n",
    "    \"\"\"\n",
    "    # Format to match training data: prompt, then separator\n",
    "    input_text = f\"{prompt}\\n\\n---\\n\\n\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "    input_len = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "    outputs_list = []\n",
    "    for i in range(n):\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(**inputs, **GEN_KWARGS, pad_token_id=tokenizer.eos_token_id)\n",
    "        # Decode only the NEW tokens (skip the prompt)\n",
    "        generated_text = tokenizer.decode(output[0][input_len:], skip_special_tokens=True)\n",
    "        outputs_list.append(generated_text)\n",
    "\n",
    "    return outputs_list\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Generate BASELINE outputs (before fine-tuning)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "baseline_outputs = {}\n",
    "for name, prompt in canary_prompts.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"BASELINE — Canary {name}\")\n",
    "    print(f\"Prompt: {prompt[:80]}...\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    samples = generate(model, tokenizer, prompt)\n",
    "    baseline_outputs[name] = samples\n",
    "\n",
    "    print(f\"\\nSample 1 of {N_SAMPLES}:\")\n",
    "    print(samples[0][:500])\n",
    "    print(\"...\" if len(samples[0]) > 500 else \"\")\n",
    "\n",
    "print(f\"\\nBaseline generation complete. {N_SAMPLES} samples per canary saved.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Load training data\n",
    "#\n",
    "# GPT-2 training format (completion-style JSONL):\n",
    "#   {\"text\": \"Write about X...\\n\\n---\\n\\nThe essay text...\"}\n",
    "#\n",
    "# Compare to Llama's format (chat-style JSONL):\n",
    "#   {\"messages\": [{\"role\": \"user\", ...}, {\"role\": \"assistant\", ...}]}\n",
    "#\n",
    "# GPT-2 doesn't know about roles or chat. It just sees a stream of text\n",
    "# and learns to predict the next token. The --- separator is the only\n",
    "# signal that says \"everything after this is the response.\"\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "train_dataset = load_dataset(\"json\", data_files=f\"{DATA_DIR}/gpt2_train.jsonl\", split=\"train\")\n",
    "val_dataset = load_dataset(\"json\", data_files=f\"{DATA_DIR}/gpt2_val.jsonl\", split=\"train\")\n",
    "\n",
    "print(f\"Training examples: {len(train_dataset)}\")\n",
    "print(f\"Validation examples: {len(val_dataset)}\")\n",
    "print(f\"\\nFirst training example (truncated):\")\n",
    "print(train_dataset[0][\"text\"][:300])\n",
    "\n",
    "\n",
    "def tokenize(example):\n",
    "    \"\"\"Tokenize a completion-style example.\n",
    "\n",
    "    Much simpler than Llama's format_chat():\n",
    "    - No chat template to apply\n",
    "    - No special role tokens\n",
    "    - Just tokenize the raw text\n",
    "    \"\"\"\n",
    "    tokenized = tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=MAX_SEQ_LEN,\n",
    "        padding=False,\n",
    "    )\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, remove_columns=train_dataset.column_names)\n",
    "val_dataset = val_dataset.map(tokenize, remove_columns=val_dataset.column_names)\n",
    "\n",
    "print(f\"\\nTokenized. First example length: {len(train_dataset[0]['input_ids'])} tokens\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ---------------------------------------------------------------------------\n# Training\n#\n# Key differences from the Llama notebook:\n# - LR is 5e-5 (vs Llama's 2e-4). Lower because full fine-tuning updates\n#   ALL 1.5B parameters. Too high and you get catastrophic forgetting.\n# - No LoRA — the Trainer updates the model directly.\n# - fp16=True to halve gradient/activation memory.\n# - adamw_bnb_8bit: 8-bit Adam via bitsandbytes. Standard Adam keeps 2 state\n#   tensors per parameter (~12GB for GPT-2-XL). 8-bit Adam compresses those\n#   to ~3GB. Combined with fp16, total VRAM fits on a T4.\n# - Should train MUCH faster than Llama (smaller model, no quantization overhead).\n# ---------------------------------------------------------------------------\n\ntraining_args = TrainingArguments(\n    output_dir=CHECKPOINT_DIR,\n    num_train_epochs=EPOCHS,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    learning_rate=LR,\n    warmup_steps=10,\n    logging_steps=1,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    save_total_limit=3,\n    fp16=True,                      # Half-precision training — saves ~3GB on gradients/activations.\n    optim=\"adamw_bnb_8bit\",         # 8-bit Adam — compresses optimizer states from ~12GB to ~3GB.\n    report_to=\"none\",\n    gradient_accumulation_steps=1,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n)\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False,  # Causal LM, not masked LM\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=data_collator,\n)\n\nprint(\"Starting training...\")\nprint(f\"  {len(train_dataset)} training examples x {EPOCHS} epochs = {len(train_dataset) * EPOCHS} steps\")\nprint(f\"  Checkpoints saved to: {CHECKPOINT_DIR}\")\n\ntrainer.train()\n\nprint(\"\\nTraining complete!\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Generate from FINE-TUNED model\n",
    "# Same canary prompts, same generate() function, same GEN_KWARGS.\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "finetuned_outputs = {}\n",
    "for name, prompt in canary_prompts.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"FINE-TUNED — Canary {name}\")\n",
    "    print(f\"Prompt: {prompt[:80]}...\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    samples = generate(model, tokenizer, prompt)\n",
    "    finetuned_outputs[name] = samples\n",
    "\n",
    "    print(f\"\\nSample 1 of {N_SAMPLES}:\")\n",
    "    print(samples[0][:500])\n",
    "    print(\"...\" if len(samples[0]) > 500 else \"\")\n",
    "\n",
    "print(f\"\\nFine-tuned generation complete. {N_SAMPLES} samples per canary saved.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Side-by-side comparison\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "for name, prompt in canary_prompts.items():\n",
    "    print(f\"\\n{'#'*70}\")\n",
    "    print(f\"  CANARY {name}\")\n",
    "    print(f\"  Prompt: {prompt}\")\n",
    "    print(f\"{'#'*70}\")\n",
    "\n",
    "    print(f\"\\n{'─'*35} BASELINE {'─'*35}\")\n",
    "    print(baseline_outputs[name][0])\n",
    "\n",
    "    print(f\"\\n{'─'*33} FINE-TUNED {'─'*33}\")\n",
    "    print(finetuned_outputs[name][0])\n",
    "\n",
    "    print()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"What to look for:\")\n",
    "print(\"  - Does the fine-tuned version sound more like you?\")\n",
    "print(\"  - Is Canary B (novel topic) closer to your voice, or only A (known topic)?\")\n",
    "print(\"  - If only A improved → memorized content, not voice.\")\n",
    "print(\"  - Compare these against the Llama notebook's outputs — which model\")\n",
    "print(\"    captured voice better? That's the RLHF question.\")\n",
    "print(\"=\"*70)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Save fine-tuned model to Google Drive\n",
    "#\n",
    "# Unlike Llama (which saves only a ~25MB LoRA adapter), this saves the\n",
    "# FULL 1.5B parameter model (~6GB). That's the tradeoff of full fine-tuning:\n",
    "# you get maximum learning capacity, but the artifact is the whole model.\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "save_path = f\"{DRIVE_BASE}/models/gpt2-voice-v1\"\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(f\"Fine-tuned model saved to: {save_path}\")\n",
    "print(f\"\\nTo reload this model later:\")\n",
    "print(f\"\"\"\\n\\\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"{save_path}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"{save_path}\")\n",
    "\"\"\")\n",
    "print(\"Note: This is the full model (~6GB), not a small adapter like Llama's LoRA.\")"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}