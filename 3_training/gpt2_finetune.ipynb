{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lowyelling/voice-fine-tuning/blob/main/3_training/gpt2_finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78MEg6CWhhg2"
      },
      "source": [
        "# GPT-2-XL Full Fine-Tuning — Voice\n",
        "\n",
        "Fine-tune GPT-2-XL (1.5B parameters) on essay pairs to capture writing voice.\n",
        "Full fine-tuning — every weight gets updated, no LoRA adapter.\n",
        "\n",
        "**Why GPT-2-XL?** It's pre-RLHF (released 2019, before alignment training existed),\n",
        "so it's a \"raw\" model. Comparing it against Llama (post-RLHF) answers the question:\n",
        "does alignment training help or hurt voice capture?\n",
        "\n",
        "**Why full fine-tuning instead of LoRA?** GPT-2-XL is small enough (1.5B params) to\n",
        "fine-tune fully on a T4 GPU. Full fine-tuning gives maximum capacity to learn voice,\n",
        "and trains in minutes. The tradeoff: higher overfitting risk on a small dataset.\n",
        "\n",
        "**Context window:** 1,024 tokens. This limits GPT-2 to short pieces (Notes, openings).\n",
        "The constraint is actually useful — it forces a clean 1:1 comparison with Llama on\n",
        "short pairs, isolating voice at the sentence level without essay-level structure.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "1. **Set runtime to GPU** (Runtime → Change runtime type → T4 GPU).\n",
        "2. That's it. No license, no token, no approval wait. GPT-2-XL is fully open.\n",
        "\n",
        "## Cell Map\n",
        "\n",
        "| Cell | What it does | When to stop |\n",
        "|------|-------------|--------------|\n",
        "| 0 | This intro | — |\n",
        "| 1 | Install dependencies | — |\n",
        "| 2 | Upload training data | SKIP for baseline only |\n",
        "| 3 | Config + mount Drive | — |\n",
        "| 4 | Load base GPT-2-XL | — |\n",
        "| 5 | Canary baselines | **STOP HERE for baseline** |\n",
        "| 5b | Save baselines to Drive | — |\n",
        "| 6 | Load training data | — |\n",
        "| 7 | Train | — |\n",
        "| 8 | Canary on fine-tuned | — |\n",
        "| 8b | Save fine-tuned outputs to Drive | — |\n",
        "| 9 | Side-by-side comparison | — |\n",
        "| 10 | Save model to Drive | — |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxkmLXZthhg3"
      },
      "source": [
        "# === Cell 1: Install dependencies ===\n",
        "# bitsandbytes is needed for the 8-bit Adam optimizer (saves ~9GB VRAM during training)\n",
        "!pip install -q transformers datasets bitsandbytes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Frdxmnizhhg3"
      },
      "source": [
        "# === Cell 2: Pull training data from GitHub ===\n",
        "\n",
        "  import os\n",
        "\n",
        "  REPO_URL = \"https://github.com/lowyelling/voice-fine-tuning.git\"\n",
        "  REPO_DIR = \"/content/voice-fine-tuning\"\n",
        "  DATA_DIR_LOCAL = \"/content/data\"\n",
        "\n",
        "  if os.path.exists(REPO_DIR):\n",
        "      !cd {REPO_DIR} && git pull\n",
        "      print(\"Pulled latest changes.\")\n",
        "  else:\n",
        "      !git clone {REPO_URL} {REPO_DIR}\n",
        "      print(\"Cloned repo.\")\n",
        "\n",
        "  # Copy JSONL files to the expected data dir\n",
        "  os.makedirs(DATA_DIR_LOCAL, exist_ok=True)\n",
        "  !cp {REPO_DIR}/1_data/pairs/*.jsonl {DATA_DIR_LOCAL}/\n",
        "\n",
        "  print(\"Training data ready:\")\n",
        "  !ls -la {DATA_DIR_LOCAL}/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dn6gMAgwhhg3"
      },
      "source": [
        "# === Cell 3: Config + mount Google Drive ===\n",
        "\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling,\n",
        ")\n",
        "from datasets import load_dataset\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive for saving the trained model\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Config\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "MODEL_ID = \"gpt2-xl\"\n",
        "\n",
        "# Training hyperparameters\n",
        "LR = 5e-5\n",
        "EPOCHS = 3\n",
        "BATCH_SIZE = 1\n",
        "MAX_SEQ_LEN = 1024\n",
        "\n",
        "# Generation hyperparameters — identical across all 4 models for fair comparison\n",
        "GEN_KWARGS = dict(\n",
        "    temperature=0.8,\n",
        "    top_p=0.9,\n",
        "    top_k=50,\n",
        "    repetition_penalty=1.1,\n",
        "    max_new_tokens=512,\n",
        "    do_sample=True,\n",
        ")\n",
        "\n",
        "N_SAMPLES = 5\n",
        "\n",
        "# Paths\n",
        "DATA_DIR = \"/content/data\"\n",
        "DRIVE_BASE = \"/content/drive/MyDrive/voice-ft\"\n",
        "CHECKPOINT_DIR = f\"{DRIVE_BASE}/checkpoints/gpt2\"\n",
        "\n",
        "print(f\"Model: {MODEL_ID}\")\n",
        "print(f\"LR={LR}, epochs={EPOCHS}, batch={BATCH_SIZE}, max_seq_len={MAX_SEQ_LEN}\")\n",
        "print(f\"Data dir: {DATA_DIR}\")\n",
        "print(f\"Checkpoints: {CHECKPOINT_DIR}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5sD6gcThhg3"
      },
      "source": [
        "# === Cell 4: Load base GPT-2-XL ===\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "\n",
        "# GPT-2 has no pad token by default. Same fix as Llama: use eos_token.\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load in fp16 to halve memory (~3GB instead of ~6GB), leaving room for training\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_ID, dtype=torch.float16)\n",
        "\n",
        "# Move to GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"  (100% trainable — full fine-tuning, no LoRA)\")\n",
        "print(f\"Model on: {device}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4cUdMMEhhg4"
      },
      "source": [
        "# === Cell 5: Generate BASELINE canary outputs (STOP HERE for baseline-only run) ===\n",
        "# ---------------------------------------------------------------------------\n",
        "# Canary prompts — fixed prompts run after every training iteration.\n",
        "# They diagnose what the model learned vs memorized.\n",
        "#\n",
        "# Source of truth: 4_experiments/canary-prompts.md\n",
        "#\n",
        "# Only A and B for GPT-2 (both short). Canary C is Llama-only because\n",
        "# it requires essay-length output that won't fit in 1,024 tokens.\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "canary_prompts = {\n",
        "    \"A\": \"Write a personal Substack Note/Tweet about class in America, told from the perspective of a Chinese first generation immigrant whose family is lower-middle class.\",\n",
        "    \"B\": \"Write a personal Substack Note/Tweet about Eileen Gu and Alyssa Liu, both winter Olympic gold medalists. Both grew up in the Bay Area, are half-asian and half-white, conceived via anonymous egg donor, and raised by a single parent. Eileen competed for China in skiing and is maximizing her influencer career while studying at Stanford. Meanwhile, Alyssa competed for the United States, took breaks from skating, and is inactive on social media.\",\n",
        "}\n",
        "\n",
        "\n",
        "def generate(model, tokenizer, prompt, n=N_SAMPLES):\n",
        "    \"\"\"Generate n samples from the model for a given prompt.\"\"\"\n",
        "    # Format to match training data: prompt, then separator\n",
        "    input_text = f\"{prompt}\\n\\n---\\n\\n\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "    input_len = inputs[\"input_ids\"].shape[1]\n",
        "\n",
        "    outputs_list = []\n",
        "    for i in range(n):\n",
        "        with torch.no_grad():\n",
        "            output = model.generate(**inputs, **GEN_KWARGS, pad_token_id=tokenizer.eos_token_id)\n",
        "        # Decode only the NEW tokens (skip the prompt)\n",
        "        generated_text = tokenizer.decode(output[0][input_len:], skip_special_tokens=True)\n",
        "        outputs_list.append(generated_text)\n",
        "\n",
        "    return outputs_list\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Generate BASELINE outputs (before fine-tuning)\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "baseline_outputs = {}\n",
        "for name, prompt in canary_prompts.items():\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"BASELINE — Canary {name}\")\n",
        "    print(f\"Prompt: {prompt[:80]}...\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    samples = generate(model, tokenizer, prompt)\n",
        "    baseline_outputs[name] = samples\n",
        "\n",
        "    print(f\"\\nSample 1 of {N_SAMPLES}:\")\n",
        "    print(samples[0][:500])\n",
        "    print(\"...\" if len(samples[0]) > 500 else \"\")\n",
        "\n",
        "print(f\"\\nBaseline generation complete. {N_SAMPLES} samples per canary saved.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 5b: Save baselines to Drive (insurance against disconnection) ===\n",
        "import json, os\n",
        "\n",
        "baseline_path = f\"{DRIVE_BASE}/baselines/gpt2_baselines.json\"\n",
        "os.makedirs(f\"{DRIVE_BASE}/baselines\", exist_ok=True)\n",
        "\n",
        "with open(baseline_path, \"w\") as f:\n",
        "    json.dump(baseline_outputs, f, indent=2)\n",
        "\n",
        "print(f\"Baselines saved to: {baseline_path}\")"
      ],
      "metadata": {
        "id": "brUfdYWphhg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHPG8F5hhhg4"
      },
      "source": [
        "# === Cell 6: Load training data ===\n",
        "\n",
        "train_dataset = load_dataset(\"json\", data_files=f\"{DATA_DIR}/gpt2_train.jsonl\", split=\"train\")\n",
        "val_dataset = load_dataset(\"json\", data_files=f\"{DATA_DIR}/gpt2_val.jsonl\", split=\"train\")\n",
        "\n",
        "print(f\"Training examples: {len(train_dataset)}\")\n",
        "print(f\"Validation examples: {len(val_dataset)}\")\n",
        "print(f\"\\nFirst training example (truncated):\")\n",
        "print(train_dataset[0][\"text\"][:300])\n",
        "\n",
        "\n",
        "def tokenize(example):\n",
        "    \"\"\"Tokenize a completion-style example.\"\"\"\n",
        "    tokenized = tokenizer(\n",
        "        example[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=MAX_SEQ_LEN,\n",
        "        padding=False,\n",
        "    )\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "    return tokenized\n",
        "\n",
        "\n",
        "train_dataset = train_dataset.map(tokenize, remove_columns=train_dataset.column_names)\n",
        "val_dataset = val_dataset.map(tokenize, remove_columns=val_dataset.column_names)\n",
        "\n",
        "print(f\"\\nTokenized. First example length: {len(train_dataset[0]['input_ids'])} tokens\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GU7a11o8hhg4"
      },
      "source": [
        "# === Cell 7: Train ===\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=CHECKPOINT_DIR,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    learning_rate=LR,\n",
        "    warmup_steps=10,\n",
        "    logging_steps=1,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"no\",         # no checkpoints — GPT-2 trains in minutes, just re-run if needed\n",
        "    fp16=False,                  # model already loaded in fp16, no need for AMP GradScaler\n",
        "    optim=\"adamw_bnb_8bit\",\n",
        "    report_to=\"none\",\n",
        "    gradient_accumulation_steps=1,\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print(\"Starting training...\")\n",
        "print(f\"  {len(train_dataset)} training examples x {EPOCHS} epochs = {len(train_dataset) * EPOCHS} steps\")\n",
        "print(f\"  Checkpoints saved to: {CHECKPOINT_DIR}\")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\nTraining complete!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0fm5L-ehhg4"
      },
      "source": [
        "# === Cell 8: Generate from FINE-TUNED model on canary prompts ===\n",
        "\n",
        "finetuned_outputs = {}\n",
        "for name, prompt in canary_prompts.items():\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"FINE-TUNED — Canary {name}\")\n",
        "    print(f\"Prompt: {prompt[:80]}...\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    samples = generate(model, tokenizer, prompt)\n",
        "    finetuned_outputs[name] = samples\n",
        "\n",
        "    print(f\"\\nSample 1 of {N_SAMPLES}:\")\n",
        "    print(samples[0][:500])\n",
        "    print(\"...\" if len(samples[0]) > 500 else \"\")\n",
        "\n",
        "print(f\"\\nFine-tuned generation complete. {N_SAMPLES} samples per canary saved.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 8b: Save fine-tuned outputs to Drive ===\n",
        "import json\n",
        "\n",
        "finetuned_path = f\"{DRIVE_BASE}/baselines/gpt2_finetuned.json\"\n",
        "\n",
        "with open(finetuned_path, \"w\") as f:\n",
        "    json.dump(finetuned_outputs, f, indent=2)\n",
        "\n",
        "print(f\"Fine-tuned outputs saved to: {finetuned_path}\")"
      ],
      "metadata": {
        "id": "BdShavdkhhg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6G89BFYthhg4"
      },
      "source": [
        "# === Cell 9: Side-by-side comparison ===\n",
        "\n",
        "for name, prompt in canary_prompts.items():\n",
        "    print(f\"\\n{'#'*70}\")\n",
        "    print(f\"  CANARY {name}\")\n",
        "    print(f\"  Prompt: {prompt}\")\n",
        "    print(f\"{'#'*70}\")\n",
        "\n",
        "    print(f\"\\n{'─'*35} BASELINE {'─'*35}\")\n",
        "    print(baseline_outputs[name][0])\n",
        "\n",
        "    print(f\"\\n{'─'*33} FINE-TUNED {'─'*33}\")\n",
        "    print(finetuned_outputs[name][0])\n",
        "\n",
        "    print()\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"What to look for:\")\n",
        "print(\"  - Does the fine-tuned version sound more like you?\")\n",
        "print(\"  - Is Canary B (novel topic) closer to your voice, or only A (known topic)?\")\n",
        "print(\"  - If only A improved → memorized content, not voice.\")\n",
        "print(\"  - Compare these against the Llama notebook's outputs — which model\")\n",
        "print(\"    captured voice better? That's the RLHF question.\")\n",
        "print(\"=\"*70)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lFYoMhdhhg4"
      },
      "source": [
        "# === Cell 10: Save fine-tuned model to Google Drive ===\n",
        "\n",
        "save_path = f\"{DRIVE_BASE}/models/gpt2-voice-v1\"\n",
        "model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)\n",
        "\n",
        "print(f\"Fine-tuned model saved to: {save_path}\")\n",
        "print(f\"\\nTo reload this model later:\")\n",
        "print(f\"\"\"\\n\\\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"{save_path}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"{save_path}\")\n",
        "\"\"\")\n",
        "print(\"Note: This is the full model (~6GB), not a small adapter like Llama's LoRA.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}