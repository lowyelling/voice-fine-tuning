{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# GPT-2-XL Full Fine-Tuning — Voice\n\nFine-tune GPT-2-XL (1.5B parameters) on essay pairs to capture writing voice.\nFull fine-tuning — every weight gets updated, no LoRA adapter.\n\n**Why GPT-2-XL?** It's pre-RLHF (released 2019, before alignment training existed),\nso it's a \"raw\" model. Comparing it against Llama (post-RLHF) answers the question:\ndoes alignment training help or hurt voice capture?\n\n**Why full fine-tuning instead of LoRA?** GPT-2-XL is small enough (1.5B params) to\nfine-tune fully on a T4 GPU. Full fine-tuning gives maximum capacity to learn voice,\nand trains in minutes. The tradeoff: higher overfitting risk on a small dataset.\n\n**Context window:** 1,024 tokens. This limits GPT-2 to short pieces (Notes, openings).\nThe constraint is actually useful — it forces a clean 1:1 comparison with Llama on\nshort pairs, isolating voice at the sentence level without essay-level structure.\n\n## Prerequisites\n\n1. **Set runtime to GPU** (Runtime → Change runtime type → T4 GPU).\n2. That's it. No license, no token, no approval wait. GPT-2-XL is fully open.\n\n## Cell Map\n\n| Cell | What it does | When to stop |\n|------|-------------|--------------|\n| 0 | This intro | — |\n| 1 | Install dependencies | — |\n| 2 | Upload training data | SKIP for baseline only |\n| 3 | Config + mount Drive | — |\n| 4 | Load base GPT-2-XL | — |\n| 5 | Canary baselines | **STOP HERE for baseline** |\n| 5b | Save baselines to Drive | — |\n| 6 | Load training data | — |\n| 7 | Train | — |\n| 8 | Canary on fine-tuned | — |\n| 8b | Save fine-tuned outputs to Drive | — |\n| 9 | Side-by-side comparison | — |\n| 10 | Save model to Drive | — |"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# === Cell 1: Install dependencies ===\n# bitsandbytes is needed for the 8-bit Adam optimizer (saves ~9GB VRAM during training)\n!pip install -q transformers datasets bitsandbytes",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# === Cell 2: Upload training data (SKIP this cell for baseline-only run) ===\n# ---------------------------------------------------------------------------\n# Upload training data from your local machine\n#\n# This opens a file picker. Select your JSONL files:\n#   - gpt2_train.jsonl\n#   - gpt2_val.jsonl\n#\n# These were generated locally by running:\n#   python3 2_scripts/format_jsonl.py\n#\n# The files live in Colab's temporary storage for this session.\n# They'll disappear when the runtime disconnects, but that's fine —\n# the trained model gets saved to Google Drive.\n# ---------------------------------------------------------------------------\n\nimport os\nfrom google.colab import files\n\nprint(\"Select your JSONL files (gpt2_train.jsonl and gpt2_val.jsonl):\")\nuploaded = files.upload()\n\nDATA_DIR = \"/content/data\"\nos.makedirs(DATA_DIR, exist_ok=True)\n\nfor filename in uploaded:\n    dest = os.path.join(DATA_DIR, filename)\n    os.rename(filename, dest)\n    print(f\"  {filename} → {dest}\")\n\nprint(f\"\\nUploaded {len(uploaded)} file(s) to {DATA_DIR}/\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# === Cell 3: Config + mount Google Drive ===\n\nimport torch\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForLanguageModeling,\n)\nfrom datasets import load_dataset\nfrom google.colab import drive\n\n# Mount Google Drive for saving the trained model\ndrive.mount(\"/content/drive\")\n\n# ---------------------------------------------------------------------------\n# Config\n# ---------------------------------------------------------------------------\n\nMODEL_ID = \"gpt2-xl\"\n\n# Training hyperparameters\nLR = 5e-5\nEPOCHS = 3\nBATCH_SIZE = 1\nMAX_SEQ_LEN = 1024\n\n# Generation hyperparameters — identical across all 4 models for fair comparison\nGEN_KWARGS = dict(\n    temperature=0.8,\n    top_p=0.9,\n    top_k=50,\n    repetition_penalty=1.1,\n    max_new_tokens=512,\n    do_sample=True,\n)\n\nN_SAMPLES = 5\n\n# Paths\nDATA_DIR = \"/content/data\"\nDRIVE_BASE = \"/content/drive/MyDrive/voice-ft\"\nCHECKPOINT_DIR = f\"{DRIVE_BASE}/checkpoints/gpt2\"\n\nprint(f\"Model: {MODEL_ID}\")\nprint(f\"LR={LR}, epochs={EPOCHS}, batch={BATCH_SIZE}, max_seq_len={MAX_SEQ_LEN}\")\nprint(f\"Data dir: {DATA_DIR}\")\nprint(f\"Checkpoints: {CHECKPOINT_DIR}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# === Cell 4: Load base GPT-2-XL ===\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n\n# GPT-2 has no pad token by default. Same fix as Llama: use eos_token.\ntokenizer.pad_token = tokenizer.eos_token\n\nmodel = AutoModelForCausalLM.from_pretrained(MODEL_ID)\n\n# Move to GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"Total parameters: {total_params:,}\")\nprint(f\"Trainable parameters: {trainable_params:,}\")\nprint(f\"  (100% trainable — full fine-tuning, no LoRA)\")\nprint(f\"Model on: {device}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# === Cell 5: Generate BASELINE canary outputs (STOP HERE for baseline-only run) ===\n# ---------------------------------------------------------------------------\n# Canary prompts — fixed prompts run after every training iteration.\n# They diagnose what the model learned vs memorized.\n#\n# Source of truth: 4_experiments/canary-prompts.md\n#\n# Only A and B for GPT-2 (both short). Canary C is Llama-only because\n# it requires essay-length output that won't fit in 1,024 tokens.\n# ---------------------------------------------------------------------------\n\ncanary_prompts = {\n    \"A\": \"Write a personal Substack Note/Tweet about class in America, told from the perspective of a Chinese first generation immigrant whose family is lower-middle class.\",\n    \"B\": \"Write a personal Substack Note/Tweet about Eileen Gu and Alyssa Liu, both winter Olympic gold medalists. Both grew up in the Bay Area, are half-asian and half-white, conceived via anonymous egg donor, and raised by a single parent. Eileen competed for China in skiing and is maximizing her influencer career while studying at Stanford. Meanwhile, Alyssa competed for the United States, took breaks from skating, and is inactive on social media.\",\n}\n\n\ndef generate(model, tokenizer, prompt, n=N_SAMPLES):\n    \"\"\"Generate n samples from the model for a given prompt.\"\"\"\n    # Format to match training data: prompt, then separator\n    input_text = f\"{prompt}\\n\\n---\\n\\n\"\n    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n    input_len = inputs[\"input_ids\"].shape[1]\n\n    outputs_list = []\n    for i in range(n):\n        with torch.no_grad():\n            output = model.generate(**inputs, **GEN_KWARGS, pad_token_id=tokenizer.eos_token_id)\n        # Decode only the NEW tokens (skip the prompt)\n        generated_text = tokenizer.decode(output[0][input_len:], skip_special_tokens=True)\n        outputs_list.append(generated_text)\n\n    return outputs_list\n\n\n# ---------------------------------------------------------------------------\n# Generate BASELINE outputs (before fine-tuning)\n# ---------------------------------------------------------------------------\n\nbaseline_outputs = {}\nfor name, prompt in canary_prompts.items():\n    print(f\"\\n{'='*60}\")\n    print(f\"BASELINE — Canary {name}\")\n    print(f\"Prompt: {prompt[:80]}...\")\n    print(f\"{'='*60}\")\n\n    samples = generate(model, tokenizer, prompt)\n    baseline_outputs[name] = samples\n\n    print(f\"\\nSample 1 of {N_SAMPLES}:\")\n    print(samples[0][:500])\n    print(\"...\" if len(samples[0]) > 500 else \"\")\n\nprint(f\"\\nBaseline generation complete. {N_SAMPLES} samples per canary saved.\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# === Cell 5b: Save baselines to Drive (insurance against disconnection) ===\nimport json, os\n\nbaseline_path = f\"{DRIVE_BASE}/baselines/gpt2_baselines.json\"\nos.makedirs(f\"{DRIVE_BASE}/baselines\", exist_ok=True)\n\nwith open(baseline_path, \"w\") as f:\n    json.dump(baseline_outputs, f, indent=2)\n\nprint(f\"Baselines saved to: {baseline_path}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# === Cell 6: Load training data ===\n\ntrain_dataset = load_dataset(\"json\", data_files=f\"{DATA_DIR}/gpt2_train.jsonl\", split=\"train\")\nval_dataset = load_dataset(\"json\", data_files=f\"{DATA_DIR}/gpt2_val.jsonl\", split=\"train\")\n\nprint(f\"Training examples: {len(train_dataset)}\")\nprint(f\"Validation examples: {len(val_dataset)}\")\nprint(f\"\\nFirst training example (truncated):\")\nprint(train_dataset[0][\"text\"][:300])\n\n\ndef tokenize(example):\n    \"\"\"Tokenize a completion-style example.\"\"\"\n    tokenized = tokenizer(\n        example[\"text\"],\n        truncation=True,\n        max_length=MAX_SEQ_LEN,\n        padding=False,\n    )\n    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n    return tokenized\n\n\ntrain_dataset = train_dataset.map(tokenize, remove_columns=train_dataset.column_names)\nval_dataset = val_dataset.map(tokenize, remove_columns=val_dataset.column_names)\n\nprint(f\"\\nTokenized. First example length: {len(train_dataset[0]['input_ids'])} tokens\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# === Cell 7: Train ===\n\ntraining_args = TrainingArguments(\n    output_dir=CHECKPOINT_DIR,\n    num_train_epochs=EPOCHS,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    learning_rate=LR,\n    warmup_steps=10,\n    logging_steps=1,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    save_total_limit=3,\n    fp16=True,\n    optim=\"adamw_bnb_8bit\",\n    report_to=\"none\",\n    gradient_accumulation_steps=1,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n)\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=data_collator,\n)\n\nprint(\"Starting training...\")\nprint(f\"  {len(train_dataset)} training examples x {EPOCHS} epochs = {len(train_dataset) * EPOCHS} steps\")\nprint(f\"  Checkpoints saved to: {CHECKPOINT_DIR}\")\n\ntrainer.train()\n\nprint(\"\\nTraining complete!\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# === Cell 8: Generate from FINE-TUNED model on canary prompts ===\n\nfinetuned_outputs = {}\nfor name, prompt in canary_prompts.items():\n    print(f\"\\n{'='*60}\")\n    print(f\"FINE-TUNED — Canary {name}\")\n    print(f\"Prompt: {prompt[:80]}...\")\n    print(f\"{'='*60}\")\n\n    samples = generate(model, tokenizer, prompt)\n    finetuned_outputs[name] = samples\n\n    print(f\"\\nSample 1 of {N_SAMPLES}:\")\n    print(samples[0][:500])\n    print(\"...\" if len(samples[0]) > 500 else \"\")\n\nprint(f\"\\nFine-tuned generation complete. {N_SAMPLES} samples per canary saved.\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# === Cell 8b: Save fine-tuned outputs to Drive ===\nimport json\n\nfinetuned_path = f\"{DRIVE_BASE}/baselines/gpt2_finetuned.json\"\n\nwith open(finetuned_path, \"w\") as f:\n    json.dump(finetuned_outputs, f, indent=2)\n\nprint(f\"Fine-tuned outputs saved to: {finetuned_path}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# === Cell 9: Side-by-side comparison ===\n\nfor name, prompt in canary_prompts.items():\n    print(f\"\\n{'#'*70}\")\n    print(f\"  CANARY {name}\")\n    print(f\"  Prompt: {prompt}\")\n    print(f\"{'#'*70}\")\n\n    print(f\"\\n{'─'*35} BASELINE {'─'*35}\")\n    print(baseline_outputs[name][0])\n\n    print(f\"\\n{'─'*33} FINE-TUNED {'─'*33}\")\n    print(finetuned_outputs[name][0])\n\n    print()\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"What to look for:\")\nprint(\"  - Does the fine-tuned version sound more like you?\")\nprint(\"  - Is Canary B (novel topic) closer to your voice, or only A (known topic)?\")\nprint(\"  - If only A improved → memorized content, not voice.\")\nprint(\"  - Compare these against the Llama notebook's outputs — which model\")\nprint(\"    captured voice better? That's the RLHF question.\")\nprint(\"=\"*70)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# === Cell 10: Save fine-tuned model to Google Drive ===\n\nsave_path = f\"{DRIVE_BASE}/models/gpt2-voice-v1\"\nmodel.save_pretrained(save_path)\ntokenizer.save_pretrained(save_path)\n\nprint(f\"Fine-tuned model saved to: {save_path}\")\nprint(f\"\\nTo reload this model later:\")\nprint(f\"\"\"\\n\\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"{save_path}\")\ntokenizer = AutoTokenizer.from_pretrained(\"{save_path}\")\n\"\"\")\nprint(\"Note: This is the full model (~6GB), not a small adapter like Llama's LoRA.\")",
   "execution_count": null,
   "outputs": []
  }
 ]
}