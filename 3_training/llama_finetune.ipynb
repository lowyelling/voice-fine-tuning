{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Llama 3.1 8B LoRA Fine-Tuning — Voice\n\nFine-tune Llama 3.1 8B on essay pairs (prompt → finished piece) to capture writing voice.\nUses LoRA so we only train ~0.1-0.5% of parameters — the rest stay frozen.\n\n**Four-way comparison:** base Llama vs fine-tuned Llama vs base GPT-2-XL vs fine-tuned GPT-2-XL.\nThis notebook handles the Llama half.\n\n## Prerequisites\n\n1. **Accept the Meta license** at [huggingface.co/meta-llama/Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct). This usually takes a few hours to approve.\n2. **Create a Hugging Face access token** at [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens). You'll paste it when prompted below.\n3. **Set runtime to GPU** (Runtime → Change runtime type → T4 GPU).\n\nTraining data is pulled from GitHub in Cell 2 — no manual upload needed.\n\n## Cell Map\n\n| Cell | What it does | When to stop |\n|------|-------------|--------------|\n| 0 | This intro | — |\n| 1 | Install dependencies | — |\n| 2 | Pull training data from GitHub | SKIP for baseline only |\n| 3 | Config + mount Drive | — |\n| 4 | Load base Llama (HF login + quantization) | — |\n| 5 | Canary baselines | **STOP HERE for baseline** |\n| 5b | Save baselines to Drive | — |\n| 6 | Apply LoRA adapter | — |\n| 7 | Load training data | — |\n| 8 | Train | — |\n| 9 | Canary on fine-tuned | — |\n| 9b | Save fine-tuned outputs to Drive | — |\n| 10 | Side-by-side comparison | — |\n| 11 | Save LoRA adapter to Drive | — |"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# === Cell 1: Install dependencies ===\n!pip install -q transformers datasets peft bitsandbytes accelerate",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# === Cell 2: Pull training data from GitHub ===\n\nimport os\n\nREPO_URL = \"https://github.com/lowyelling/voice-fine-tuning.git\"\nBRANCH = \"15-pairs\"  # Change this when you merge to main\nREPO_DIR = \"/content/voice-fine-tuning\"\nDATA_DIR_LOCAL = \"/content/data\"\n\nif os.path.exists(REPO_DIR):\n    !cd {REPO_DIR} && git checkout {BRANCH} && git pull\n    print(f\"Pulled latest from {BRANCH}.\")\nelse:\n    !git clone -b {BRANCH} {REPO_URL} {REPO_DIR}\n    print(f\"Cloned repo on branch {BRANCH}.\")\n\n# Copy JSONL files to the expected data dir\nos.makedirs(DATA_DIR_LOCAL, exist_ok=True)\n!cp {REPO_DIR}/1_data/jsonl/llama_*.jsonl {DATA_DIR_LOCAL}/\n\nprint(\"\\nTraining data ready:\")\n!ls -la {DATA_DIR_LOCAL}/",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# === Cell 3: Config + mount Google Drive ===\n\nimport torch\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    BitsAndBytesConfig,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForLanguageModeling,\n)\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom datasets import load_dataset\nfrom google.colab import drive\n\n# Mount Google Drive for saving checkpoints and the final adapter\ndrive.mount(\"/content/drive\")\n\n# ---------------------------------------------------------------------------\n# Config\n# ---------------------------------------------------------------------------\n\nMODEL_ID = \"meta-llama/Llama-3.1-8B-Instruct\"\n\n# LoRA hyperparameters\nLORA_R = 16\nLORA_ALPHA = 32\n\n# Training hyperparameters\nLR = 1e-4\nEPOCHS = 3\nBATCH_SIZE = 1\nMAX_SEQ_LEN = 8192\n\n# Generation hyperparameters — identical across all 4 models for fair comparison\nGEN_KWARGS = dict(\n    temperature=0.8,\n    top_p=0.9,\n    top_k=50,\n    repetition_penalty=1.1,\n    max_new_tokens=1024,\n    do_sample=True,\n)\n\nN_SAMPLES = 5\n\n# Paths\nDATA_DIR = \"/content/data\"\nDRIVE_BASE = \"/content/drive/MyDrive/voice-ft\"\nCHECKPOINT_DIR = f\"{DRIVE_BASE}/checkpoints/llama\"\n\nprint(f\"Model: {MODEL_ID}\")\nprint(f\"LoRA rank={LORA_R}, alpha={LORA_ALPHA}\")\nprint(f\"LR={LR}, epochs={EPOCHS}, batch={BATCH_SIZE}, max_seq_len={MAX_SEQ_LEN}\")\nprint(f\"Data dir: {DATA_DIR}\")\nprint(f\"Checkpoints: {CHECKPOINT_DIR}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# === Cell 4: Load base Llama (HF login + 4-bit quantization) ===\n\nfrom huggingface_hub import login\nfrom google.colab import userdata\n\nlogin(token=userdata.get(\"HF_TOKEN\"))\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,\n)\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\nprint(\"Loading model in 4-bit quantization...\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_ID,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n)\n\nmodel = prepare_model_for_kbit_training(model)\n\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"Total parameters: {total_params:,}\")\nprint(f\"Trainable parameters (before LoRA): {trainable_params:,}\")\nprint(f\"Model loaded on: {model.device}\")\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# === Cell 5: Generate BASELINE canary outputs (STOP HERE for baseline-only run) ===\n# ---------------------------------------------------------------------------\n# Source of truth: 4_experiments/canary-prompts.md\n#\n#   A: known topic, short → compare against actual Note\n#   B: novel topic, short → tests voice generalization\n#   C: known topic, long  → tests essay-level architecture (Llama only)\n# ---------------------------------------------------------------------------\n\ncanary_prompts = {\n    \"A\": \"Write a personal Substack Note/Tweet about class in America, told from the perspective of a Chinese first generation immigrant whose family is lower-middle class.\",\n    \"B\": \"Write a personal Substack Note/Tweet about Eileen Gu and Alyssa Liu, both winter Olympic gold medalists. Both grew up in the Bay Area, are half-asian and half-white, conceived via anonymous egg donor, and raised by a single parent. Eileen competed for China in skiing and is maximizing her influencer career while studying at Stanford. Meanwhile, Alyssa competed for the United States, took breaks from skating, and is inactive on social media.\",\n    \"C\": \"Write an essay about Jacques Ellul as a forgotten prophet of propaganda and technological conformity.\",\n}\n\n\ndef generate(model, tokenizer, prompt, n=N_SAMPLES):\n    \"\"\"Generate n samples from the model for a given prompt.\"\"\"\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n    input_text = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True,\n    )\n\n    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n    input_len = inputs[\"input_ids\"].shape[1]\n\n    outputs_list = []\n    for i in range(n):\n        with torch.no_grad():\n            output = model.generate(**inputs, **GEN_KWARGS)\n        generated_text = tokenizer.decode(output[0][input_len:], skip_special_tokens=True)\n        outputs_list.append(generated_text)\n\n    return outputs_list\n\n\n# ---------------------------------------------------------------------------\n# Generate BASELINE outputs (before fine-tuning)\n# ---------------------------------------------------------------------------\n\nbaseline_outputs = {}\nfor name, prompt in canary_prompts.items():\n    print(f\"\\n{'='*60}\")\n    print(f\"BASELINE — Canary {name}\")\n    print(f\"Prompt: {prompt[:80]}...\")\n    print(f\"{'='*60}\")\n\n    samples = generate(model, tokenizer, prompt)\n    baseline_outputs[name] = samples\n\n    print(f\"\\nSample 1 of {N_SAMPLES}:\")\n    print(samples[0][:500])\n    print(\"...\" if len(samples[0]) > 500 else \"\")\n\nprint(f\"\\nBaseline generation complete. {N_SAMPLES} samples per canary saved.\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# === Cell 5b: Save baselines to Drive (insurance against disconnection) ===\nimport json, os\n\nbaseline_path = f\"{DRIVE_BASE}/baselines/llama_baselines.json\"\nos.makedirs(f\"{DRIVE_BASE}/baselines\", exist_ok=True)\n\nwith open(baseline_path, \"w\") as f:\n    json.dump(baseline_outputs, f, indent=2)\n\nprint(f\"Baselines saved to: {baseline_path}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# === Cell 6: Apply LoRA adapter ===\n\nlora_config = LoraConfig(\n    r=LORA_R,\n    lora_alpha=LORA_ALPHA,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\nmodel = get_peft_model(model, lora_config)\n\n# Should print ~0.1% trainable. If 0%, something went wrong.\nmodel.print_trainable_parameters()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# === Cell 7: Load training data ===\n\ntrain_path = f\"{DATA_DIR}/llama_train.jsonl\"\nval_path = f\"{DATA_DIR}/llama_val.jsonl\"\n\ntrain_dataset = load_dataset(\"json\", data_files=train_path, split=\"train\")\nval_dataset = load_dataset(\"json\", data_files=val_path, split=\"train\")\n\nprint(f\"Training examples: {len(train_dataset)}\")\nprint(f\"Validation examples: {len(val_dataset)}\")\nprint(f\"\\nFirst training example messages[0] (user prompt):\")\nprint(train_dataset[0][\"messages\"][0][\"content\"][:200])\n\n\ndef format_chat(example):\n    \"\"\"Convert a chat-style example into tokenized input for training.\n    Only compute loss on the assistant response, not the prompt/template.\"\"\"\n    # Full conversation (user + assistant)\n    full_text = tokenizer.apply_chat_template(\n        example[\"messages\"],\n        tokenize=False,\n        add_generation_prompt=False,\n    )\n\n    # Prompt only (user turn + generation prompt marker)\n    prompt_text = tokenizer.apply_chat_template(\n        example[\"messages\"][:1],  # just the user message\n        tokenize=False,\n        add_generation_prompt=True,\n    )\n\n    tokenized = tokenizer(\n        full_text,\n        truncation=True,\n        max_length=MAX_SEQ_LEN,\n        padding=False,\n    )\n\n    # Mask loss on prompt tokens — only train on assistant response\n    prompt_len = len(tokenizer(prompt_text, truncation=True, max_length=MAX_SEQ_LEN)[\"input_ids\"])\n    labels = tokenized[\"input_ids\"].copy()\n    labels[:prompt_len] = [-100] * prompt_len\n    tokenized[\"labels\"] = labels\n\n    return tokenized\n\n\ntrain_dataset = train_dataset.map(format_chat, remove_columns=train_dataset.column_names)\nval_dataset = val_dataset.map(format_chat, remove_columns=val_dataset.column_names)\n\n# Verify masking worked — show total vs trained tokens\ntotal_tokens = len(train_dataset[0][\"input_ids\"])\ntrained_tokens = sum(1 for l in train_dataset[0][\"labels\"] if l != -100)\nprint(f\"\\nTokenized. First example: {total_tokens} total tokens, {trained_tokens} trained (assistant only)\")\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# === Cell 8: Train ===\n\nfrom transformers import DataCollatorForSeq2Seq\n\ntraining_args = TrainingArguments(\n    output_dir=CHECKPOINT_DIR,\n    num_train_epochs=EPOCHS,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    learning_rate=LR,\n    warmup_steps=2,\n    logging_steps=1,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    save_total_limit=3,\n    fp16=True,\n    report_to=\"none\",\n    gradient_accumulation_steps=1,\n    gradient_checkpointing=True,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n)\n\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer=tokenizer,\n    padding=True,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=data_collator,\n)\n\nprint(\"Starting training...\")\nprint(f\"  {len(train_dataset)} training examples x {EPOCHS} epochs = {len(train_dataset) * EPOCHS} steps\")\nprint(f\"  Warmup: 2 steps\")\nprint(f\"  Gradient checkpointing: ON (trades compute for VRAM)\")\nprint(f\"  Checkpoints saved to: {CHECKPOINT_DIR}\")\n\ntrainer.train()\n\nprint(\"\\nTraining complete!\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# === Cell 9: Generate from FINE-TUNED model on canary prompts ===\n\nfinetuned_outputs = {}\nfor name, prompt in canary_prompts.items():\n    print(f\"\\n{'='*60}\")\n    print(f\"FINE-TUNED — Canary {name}\")\n    print(f\"Prompt: {prompt[:80]}...\")\n    print(f\"{'='*60}\")\n\n    samples = generate(model, tokenizer, prompt)\n    finetuned_outputs[name] = samples\n\n    print(f\"\\nSample 1 of {N_SAMPLES}:\")\n    print(samples[0][:500])\n    print(\"...\" if len(samples[0]) > 500 else \"\")\n\nprint(f\"\\nFine-tuned generation complete. {N_SAMPLES} samples per canary saved.\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# === Cell 9b: Save fine-tuned outputs to Drive ===\nimport json\n\nfinetuned_path = f\"{DRIVE_BASE}/baselines/llama_finetuned.json\"\n\nwith open(finetuned_path, \"w\") as f:\n    json.dump(finetuned_outputs, f, indent=2)\n\nprint(f\"Fine-tuned outputs saved to: {finetuned_path}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# === Cell 10: Side-by-side comparison ===\n\nfor name, prompt in canary_prompts.items():\n    print(f\"\\n{'#'*70}\")\n    print(f\"  CANARY {name}\")\n    print(f\"  Prompt: {prompt}\")\n    print(f\"{'#'*70}\")\n\n    print(f\"\\n{'─'*35} BASELINE {'─'*35}\")\n    print(baseline_outputs[name][0])\n\n    print(f\"\\n{'─'*33} FINE-TUNED {'─'*33}\")\n    print(finetuned_outputs[name][0])\n\n    print()\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"What to look for:\")\nprint(\"  - Does the fine-tuned version sound more like you?\")\nprint(\"  - Is Canary B (novel topic) closer to your voice, or only A (known topic)?\")\nprint(\"  - If only A improved → the model memorized content, not voice.\")\nprint(\"  - If A and B improved but C (long) didn't → learned sentence voice, not structure.\")\nprint(\"=\"*70)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# === Cell 11: Save LoRA adapter to Google Drive ===\n\nadapter_path = f\"{DRIVE_BASE}/adapters/llama-voice-v1\"\nmodel.save_pretrained(adapter_path)\ntokenizer.save_pretrained(adapter_path)\n\nprint(f\"LoRA adapter saved to: {adapter_path}\")\nprint(f\"\\nTo reload this adapter later:\")\nprint(f\"\"\"\\n\\\nfrom peft import PeftModel\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nimport torch\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,\n)\nbase_model = AutoModelForCausalLM.from_pretrained(\n    \"{MODEL_ID}\",\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n)\n\nmodel = PeftModel.from_pretrained(base_model, \"{adapter_path}\")\ntokenizer = AutoTokenizer.from_pretrained(\"{adapter_path}\")\n\"\"\")",
   "execution_count": null,
   "outputs": []
  }
 ]
}