{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lowyelling/voice-fine-tuning/blob/main/3_training/llama_finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPceO5DThqow"
      },
      "source": [
        "# Llama 3.1 8B LoRA Fine-Tuning — Voice\n",
        "\n",
        "Fine-tune Llama 3.1 8B on essay pairs (prompt → finished piece) to capture writing voice.\n",
        "Uses LoRA so we only train ~0.1-0.5% of parameters — the rest stay frozen.\n",
        "\n",
        "**Four-way comparison:** base Llama vs fine-tuned Llama vs base GPT-2-XL vs fine-tuned GPT-2-XL.\n",
        "This notebook handles the Llama half.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "1. **Accept the Meta license** at [huggingface.co/meta-llama/Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct). This usually takes a few hours to approve.\n",
        "2. **Create a Hugging Face access token** at [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens). You'll paste it when prompted below.\n",
        "3. **Set runtime to GPU** (Runtime → Change runtime type → T4 GPU).\n",
        "\n",
        "Training data gets uploaded in the cells below — no need to manually put files on Drive first.\n",
        "\n",
        "## Cell Map\n",
        "\n",
        "| Cell | What it does | When to stop |\n",
        "|------|-------------|--------------|\n",
        "| 0 | This intro | — |\n",
        "| 1 | Install dependencies | — |\n",
        "| 2 | Upload training data | SKIP for baseline only |\n",
        "| 3 | Config + mount Drive | — |\n",
        "| 4 | Load base Llama (HF login + quantization) | — |\n",
        "| 5 | Canary baselines | **STOP HERE for baseline** |\n",
        "| 5b | Save baselines to Drive | — |\n",
        "| 6 | Apply LoRA adapter | — |\n",
        "| 7 | Load training data | — |\n",
        "| 8 | Train | — |\n",
        "| 9 | Canary on fine-tuned | — |\n",
        "| 9b | Save fine-tuned outputs to Drive | — |\n",
        "| 10 | Side-by-side comparison | — |\n",
        "| 11 | Save LoRA adapter to Drive | — |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBLSkJBkhqox"
      },
      "source": [
        "# === Cell 1: Install dependencies ===\n",
        "!pip install -q transformers datasets peft bitsandbytes accelerate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 2: Pull training data from GitHub ===\n",
        "\n",
        "import os\n",
        "\n",
        "REPO_URL = \"https://github.com/lowyelling/voice-fine-tuning.git\"\n",
        "BRANCH = \"15-pairs\"  # Change this when you merge to main\n",
        "REPO_DIR = \"/content/voice-fine-tuning\"\n",
        "DATA_DIR_LOCAL = \"/content/data\"\n",
        "\n",
        "if os.path.exists(REPO_DIR):\n",
        "    !cd {REPO_DIR} && git checkout {BRANCH} && git pull\n",
        "    print(f\"Pulled latest from {BRANCH}.\")\n",
        "else:\n",
        "    !git clone -b {BRANCH} {REPO_URL} {REPO_DIR}\n",
        "    print(f\"Cloned repo on branch {BRANCH}.\")\n",
        "\n",
        "# Copy JSONL files to the expected data dir\n",
        "os.makedirs(DATA_DIR_LOCAL, exist_ok=True)\n",
        "!cp {REPO_DIR}/1_data/jsonl/llama_*.jsonl {DATA_DIR_LOCAL}/\n",
        "\n",
        "print(\"\\nTraining data ready:\")\n",
        "!ls -la {DATA_DIR_LOCAL}/"
      ],
      "metadata": {
        "id": "iX2-E_Uihqox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qicgRrQlhqox"
      },
      "source": [
        "# === Cell 3: Config + mount Google Drive ===\n",
        "\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from datasets import load_dataset\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive for saving checkpoints and the final adapter\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Config\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "MODEL_ID = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "\n",
        "# LoRA hyperparameters\n",
        "LORA_R = 16\n",
        "LORA_ALPHA = 32\n",
        "\n",
        "# Training hyperparameters\n",
        "LR = 2e-4\n",
        "EPOCHS = 3\n",
        "BATCH_SIZE = 1\n",
        "MAX_SEQ_LEN = 2048\n",
        "\n",
        "# Generation hyperparameters — identical across all 4 models for fair comparison\n",
        "GEN_KWARGS = dict(\n",
        "    temperature=0.8,\n",
        "    top_p=0.9,\n",
        "    top_k=50,\n",
        "    repetition_penalty=1.1,\n",
        "    max_new_tokens=1024,\n",
        "    do_sample=True,\n",
        ")\n",
        "\n",
        "N_SAMPLES = 5\n",
        "\n",
        "# Paths\n",
        "DATA_DIR = \"/content/data\"\n",
        "DRIVE_BASE = \"/content/drive/MyDrive/voice-ft\"\n",
        "CHECKPOINT_DIR = f\"{DRIVE_BASE}/checkpoints/llama\"\n",
        "\n",
        "print(f\"Model: {MODEL_ID}\")\n",
        "print(f\"LoRA rank={LORA_R}, alpha={LORA_ALPHA}\")\n",
        "print(f\"LR={LR}, epochs={EPOCHS}, batch={BATCH_SIZE}, max_seq_len={MAX_SEQ_LEN}\")\n",
        "print(f\"Data dir: {DATA_DIR}\")\n",
        "print(f\"Checkpoints: {CHECKPOINT_DIR}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmGZVXKmhqox"
      },
      "source": [
        "# === Cell 4: Load base Llama (HF login + 4-bit quantization) ===\n",
        "\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "login(token=userdata.get(\"HF_TOKEN\"))\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "print(\"Loading model in 4-bit quantization...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters (before LoRA): {trainable_params:,}\")\n",
        "print(f\"Model loaded on: {model.device}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tn5T_mvghqoy"
      },
      "source": [
        "# === Cell 5: Generate BASELINE canary outputs (STOP HERE for baseline-only run) ===\n",
        "# ---------------------------------------------------------------------------\n",
        "# Source of truth: 4_experiments/canary-prompts.md\n",
        "#\n",
        "#   A: known topic, short → compare against actual Note\n",
        "#   B: novel topic, short → tests voice generalization\n",
        "#   C: known topic, long  → tests essay-level architecture (Llama only)\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "canary_prompts = {\n",
        "    \"A\": \"Write a personal Substack Note/Tweet about class in America, told from the perspective of a Chinese first generation immigrant whose family is lower-middle class.\",\n",
        "    \"B\": \"Write a personal Substack Note/Tweet about Eileen Gu and Alyssa Liu, both winter Olympic gold medalists. Both grew up in the Bay Area, are half-asian and half-white, conceived via anonymous egg donor, and raised by a single parent. Eileen competed for China in skiing and is maximizing her influencer career while studying at Stanford. Meanwhile, Alyssa competed for the United States, took breaks from skating, and is inactive on social media.\",\n",
        "    \"C\": \"Write an essay about Jacques Ellul as a forgotten prophet of propaganda and technological conformity.\",\n",
        "}\n",
        "\n",
        "\n",
        "def generate(model, tokenizer, prompt, n=N_SAMPLES):\n",
        "    \"\"\"Generate n samples from the model for a given prompt.\"\"\"\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    input_text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True,\n",
        "    )\n",
        "\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "    input_len = inputs[\"input_ids\"].shape[1]\n",
        "\n",
        "    outputs_list = []\n",
        "    for i in range(n):\n",
        "        with torch.no_grad():\n",
        "            output = model.generate(**inputs, **GEN_KWARGS)\n",
        "        generated_text = tokenizer.decode(output[0][input_len:], skip_special_tokens=True)\n",
        "        outputs_list.append(generated_text)\n",
        "\n",
        "    return outputs_list\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Generate BASELINE outputs (before fine-tuning)\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "baseline_outputs = {}\n",
        "for name, prompt in canary_prompts.items():\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"BASELINE — Canary {name}\")\n",
        "    print(f\"Prompt: {prompt[:80]}...\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    samples = generate(model, tokenizer, prompt)\n",
        "    baseline_outputs[name] = samples\n",
        "\n",
        "    print(f\"\\nSample 1 of {N_SAMPLES}:\")\n",
        "    print(samples[0][:500])\n",
        "    print(\"...\" if len(samples[0]) > 500 else \"\")\n",
        "\n",
        "print(f\"\\nBaseline generation complete. {N_SAMPLES} samples per canary saved.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 5b: Save baselines to Drive (insurance against disconnection) ===\n",
        "import json, os\n",
        "\n",
        "baseline_path = f\"{DRIVE_BASE}/baselines/llama_baselines.json\"\n",
        "os.makedirs(f\"{DRIVE_BASE}/baselines\", exist_ok=True)\n",
        "\n",
        "with open(baseline_path, \"w\") as f:\n",
        "    json.dump(baseline_outputs, f, indent=2)\n",
        "\n",
        "print(f\"Baselines saved to: {baseline_path}\")"
      ],
      "metadata": {
        "id": "tE3TvWKwhqoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMtWmVEWhqoy"
      },
      "source": [
        "# === Cell 6: Apply LoRA adapter ===\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Should print ~0.1% trainable. If 0%, something went wrong.\n",
        "model.print_trainable_parameters()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1FwBQ1Mhqoy"
      },
      "source": [
        "# === Cell 7: Load training data ===\n",
        "\n",
        "train_path = f\"{DATA_DIR}/llama_train.jsonl\"\n",
        "val_path = f\"{DATA_DIR}/llama_val.jsonl\"\n",
        "\n",
        "train_dataset = load_dataset(\"json\", data_files=train_path, split=\"train\")\n",
        "val_dataset = load_dataset(\"json\", data_files=val_path, split=\"train\")\n",
        "\n",
        "print(f\"Training examples: {len(train_dataset)}\")\n",
        "print(f\"Validation examples: {len(val_dataset)}\")\n",
        "print(f\"\\nFirst training example messages[0] (user prompt):\")\n",
        "print(train_dataset[0][\"messages\"][0][\"content\"][:200])\n",
        "\n",
        "\n",
        "def format_chat(example):\n",
        "    \"\"\"Convert a chat-style example into tokenized input for training.\n",
        "    Only compute loss on the assistant response, not the prompt/template.\"\"\"\n",
        "    # Full conversation (user + assistant)\n",
        "    full_text = tokenizer.apply_chat_template(\n",
        "        example[\"messages\"],\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=False,\n",
        "    )\n",
        "\n",
        "    # Prompt only (user turn + generation prompt marker)\n",
        "    prompt_text = tokenizer.apply_chat_template(\n",
        "        example[\"messages\"][:1],  # just the user message\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True,\n",
        "    )\n",
        "\n",
        "    tokenized = tokenizer(\n",
        "        full_text,\n",
        "        truncation=True,\n",
        "        max_length=MAX_SEQ_LEN,\n",
        "        padding=False,\n",
        "    )\n",
        "\n",
        "    # Mask loss on prompt tokens — only train on assistant response\n",
        "    prompt_len = len(tokenizer(prompt_text, truncation=True, max_length=MAX_SEQ_LEN)[\"input_ids\"])\n",
        "    labels = tokenized[\"input_ids\"].copy()\n",
        "    labels[:prompt_len] = [-100] * prompt_len\n",
        "    tokenized[\"labels\"] = labels\n",
        "\n",
        "    return tokenized\n",
        "\n",
        "\n",
        "train_dataset = train_dataset.map(format_chat, remove_columns=train_dataset.column_names)\n",
        "val_dataset = val_dataset.map(format_chat, remove_columns=val_dataset.column_names)\n",
        "\n",
        "# Verify masking worked — show total vs trained tokens\n",
        "total_tokens = len(train_dataset[0][\"input_ids\"])\n",
        "trained_tokens = sum(1 for l in train_dataset[0][\"labels\"] if l != -100)\n",
        "print(f\"\\nTokenized. First example: {total_tokens} total tokens, {trained_tokens} trained (assistant only)\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuA73ZPkhqoy"
      },
      "source": [
        "# === Cell 8: Train ===\n",
        "\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=CHECKPOINT_DIR,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    learning_rate=LR,\n",
        "    warmup_steps=2,\n",
        "    logging_steps=1,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=3,\n",
        "    fp16=True,\n",
        "    report_to=\"none\",\n",
        "    gradient_accumulation_steps=1,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer,\n",
        "    padding=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print(\"Starting training...\")\n",
        "print(f\"  {len(train_dataset)} training examples x {EPOCHS} epochs = {len(train_dataset) * EPOCHS} steps\")\n",
        "print(f\"  Warmup: 2 steps\")\n",
        "print(f\"  Checkpoints saved to: {CHECKPOINT_DIR}\")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\nTraining complete!\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yds8u2-phqoy"
      },
      "source": [
        "# === Cell 9: Generate from FINE-TUNED model on canary prompts ===\n",
        "\n",
        "finetuned_outputs = {}\n",
        "for name, prompt in canary_prompts.items():\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"FINE-TUNED — Canary {name}\")\n",
        "    print(f\"Prompt: {prompt[:80]}...\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    samples = generate(model, tokenizer, prompt)\n",
        "    finetuned_outputs[name] = samples\n",
        "\n",
        "    print(f\"\\nSample 1 of {N_SAMPLES}:\")\n",
        "    print(samples[0][:500])\n",
        "    print(\"...\" if len(samples[0]) > 500 else \"\")\n",
        "\n",
        "print(f\"\\nFine-tuned generation complete. {N_SAMPLES} samples per canary saved.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 9b: Save fine-tuned outputs to Drive ===\n",
        "import json\n",
        "\n",
        "finetuned_path = f\"{DRIVE_BASE}/baselines/llama_finetuned.json\"\n",
        "\n",
        "with open(finetuned_path, \"w\") as f:\n",
        "    json.dump(finetuned_outputs, f, indent=2)\n",
        "\n",
        "print(f\"Fine-tuned outputs saved to: {finetuned_path}\")"
      ],
      "metadata": {
        "id": "sfRXYIHVhqoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6G3FjQYQhqoy"
      },
      "source": [
        "# === Cell 10: Side-by-side comparison ===\n",
        "\n",
        "for name, prompt in canary_prompts.items():\n",
        "    print(f\"\\n{'#'*70}\")\n",
        "    print(f\"  CANARY {name}\")\n",
        "    print(f\"  Prompt: {prompt}\")\n",
        "    print(f\"{'#'*70}\")\n",
        "\n",
        "    print(f\"\\n{'─'*35} BASELINE {'─'*35}\")\n",
        "    print(baseline_outputs[name][0])\n",
        "\n",
        "    print(f\"\\n{'─'*33} FINE-TUNED {'─'*33}\")\n",
        "    print(finetuned_outputs[name][0])\n",
        "\n",
        "    print()\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"What to look for:\")\n",
        "print(\"  - Does the fine-tuned version sound more like you?\")\n",
        "print(\"  - Is Canary B (novel topic) closer to your voice, or only A (known topic)?\")\n",
        "print(\"  - If only A improved → the model memorized content, not voice.\")\n",
        "print(\"  - If A and B improved but C (long) didn't → learned sentence voice, not structure.\")\n",
        "print(\"=\"*70)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ci5Wezkkhqoy"
      },
      "source": [
        "# === Cell 11: Save LoRA adapter to Google Drive ===\n",
        "\n",
        "adapter_path = f\"{DRIVE_BASE}/adapters/llama-voice-v1\"\n",
        "model.save_pretrained(adapter_path)\n",
        "tokenizer.save_pretrained(adapter_path)\n",
        "\n",
        "print(f\"LoRA adapter saved to: {adapter_path}\")\n",
        "print(f\"\\nTo reload this adapter later:\")\n",
        "print(f\"\"\"\\n\\\n",
        "from peft import PeftModel\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"{MODEL_ID}\",\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "model = PeftModel.from_pretrained(base_model, \"{adapter_path}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"{adapter_path}\")\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}