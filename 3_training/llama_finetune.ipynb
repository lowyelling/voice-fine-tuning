{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Llama 3.1 8B LoRA Fine-Tuning — Voice\n\nFine-tune Llama 3.1 8B on essay pairs (prompt → finished piece) to capture writing voice.\nUses LoRA so we only train ~0.1-0.5% of parameters — the rest stay frozen.\n\n**Four-way comparison:** base Llama vs fine-tuned Llama vs base GPT-2-XL vs fine-tuned GPT-2-XL.\nThis notebook handles the Llama half.\n\n## Prerequisites\n\n1. **Accept the Meta license** at [huggingface.co/meta-llama/Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct). This usually takes a few hours to approve.\n2. **Create a Hugging Face access token** at [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens). You'll paste it when prompted below.\n3. **Set runtime to GPU** (Runtime → Change runtime type → T4 GPU).\n\nTraining data gets uploaded in the cells below — no need to manually put files on Drive first."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install -q transformers datasets peft bitsandbytes accelerate"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ---------------------------------------------------------------------------\n# Upload training data from your local machine\n#\n# This opens a file picker. Select your JSONL files:\n#   - llama_train.jsonl\n#   - llama_val.jsonl\n#\n# These were generated locally by running:\n#   python3 2_scripts/format_jsonl.py\n#\n# The files live in Colab's temporary storage (/content/) for this session.\n# They'll disappear when the runtime disconnects, but that's fine — the\n# trained adapter gets saved to Google Drive, and you can re-upload data\n# for the next run.\n# ---------------------------------------------------------------------------\n\nimport os\nfrom google.colab import files\n\nprint(\"Select your JSONL files (llama_train.jsonl and llama_val.jsonl):\")\nuploaded = files.upload()\n\n# Move uploaded files to a consistent location\nDATA_DIR_LOCAL = \"/content/data\"\nos.makedirs(DATA_DIR_LOCAL, exist_ok=True)\n\nfor filename in uploaded:\n    dest = os.path.join(DATA_DIR_LOCAL, filename)\n    os.rename(filename, dest)\n    print(f\"  {filename} → {dest}\")\n\nprint(f\"\\nUploaded {len(uploaded)} file(s) to {DATA_DIR_LOCAL}/\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import torch\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    BitsAndBytesConfig,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForLanguageModeling,\n)\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom datasets import load_dataset\nfrom google.colab import drive\n\n# Mount Google Drive for saving checkpoints and the final adapter\ndrive.mount(\"/content/drive\")\n\n# ---------------------------------------------------------------------------\n# Config — all hyperparameters in one place so you can tweak without hunting\n# ---------------------------------------------------------------------------\n\n# We use Instruct (not base) because our training data is chat-style JSONL:\n# {\"messages\": [{\"role\": \"user\", ...}, {\"role\": \"assistant\", ...}]}\n# The Instruct model already knows the chat template, so apply_chat_template()\n# produces the right special tokens. Base model would ignore them.\nMODEL_ID = \"meta-llama/Llama-3.1-8B-Instruct\"\n\n# LoRA hyperparameters\nLORA_R = 16       # Rank — how many dimensions the adapter adds. 16 is moderate.\nLORA_ALPHA = 32   # Scaling factor. Rule of thumb: alpha = 2 * rank.\n\n# Training hyperparameters\nLR = 2e-4         # Learning rate for LoRA. Higher than full fine-tuning because\n                   # we're only updating a tiny fraction of weights.\nEPOCHS = 3\nBATCH_SIZE = 1     # T4 has 16GB VRAM — batch of 1 is safest with 8B model.\nMAX_SEQ_LEN = 2048 # Llama supports 128K, but 2048 covers most essays and saves memory.\n\n# Generation hyperparameters — identical across all 4 models for fair comparison\nGEN_KWARGS = dict(\n    temperature=0.8,\n    top_p=0.9,\n    top_k=50,\n    repetition_penalty=1.1,\n    max_new_tokens=1024,\n    do_sample=True,\n)\n\nN_SAMPLES = 5  # Generate 5 samples per prompt, compare the best from each model.\n\n# Paths\n# Training data: uploaded to Colab local storage (cell above)\nDATA_DIR = \"/content/data\"\n# Checkpoints and adapters: saved to Google Drive (persists across sessions)\nDRIVE_BASE = \"/content/drive/MyDrive/voice-ft\"\nCHECKPOINT_DIR = f\"{DRIVE_BASE}/checkpoints/llama\"\n\nprint(f\"Model: {MODEL_ID}\")\nprint(f\"LoRA rank={LORA_R}, alpha={LORA_ALPHA}\")\nprint(f\"LR={LR}, epochs={EPOCHS}, batch={BATCH_SIZE}, max_seq_len={MAX_SEQ_LEN}\")\nprint(f\"Data dir: {DATA_DIR}\")\nprint(f\"Checkpoints: {CHECKPOINT_DIR}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# This opens a prompt where you paste your HF access token.\n",
    "# The token is NOT stored in the notebook — it lives in your Colab session only.\n",
    "login()\n",
    "\n",
    "# 4-bit quantization config: loads the 8B model into ~4-5GB VRAM instead of ~16GB.\n",
    "# nf4 (normalized float 4) is the best quantization type for fine-tuning.\n",
    "# double_quant quantizes the quantization constants too — extra memory savings.\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "# Llama doesn't have a pad token by default. We set it to eos_token so padding\n",
    "# doesn't introduce a new token the model hasn't seen.\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"  # Right-padding for causal LM (left would mask real tokens)\n",
    "\n",
    "print(\"Loading model in 4-bit quantization...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",  # Automatically places layers across available devices\n",
    ")\n",
    "\n",
    "# prepare_model_for_kbit_training does two things:\n",
    "# 1. Freezes the quantized base weights so we don't try to backprop through 4-bit values\n",
    "# 2. Casts certain layers to float32 for stable gradient computation\n",
    "# Without this, training on a quantized model will silently produce garbage.\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Count parameters to verify model loaded correctly\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters (before LoRA): {trainable_params:,}\")\n",
    "print(f\"Model loaded on: {model.device}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Canary prompts — fixed prompts run after every training iteration.\n",
    "# They diagnose what the model learned vs memorized.\n",
    "#\n",
    "#   A: known topic, short → compare against actual essay (did it learn your voice?)\n",
    "#   B: novel topic, short → tests generalization (voice without memorized content?)\n",
    "#   C: known topic, long  → tests essay-level architecture (Llama only)\n",
    "#\n",
    "# If A sounds like you but B doesn't → memorized content, not voice.\n",
    "# If A and B sound right but C doesn't → learned sentence-level voice, not structure.\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "canary_prompts = {\n",
    "    \"A\": \"Write a short essay about why writing transforms how you think. Thesis: writing is not a record of thought — it is the act of thinking itself.\",\n",
    "    \"B\": \"Write a short essay about what rock climbing teaches you about fear. Thesis: the hardest part is not the wall — it is trusting your hands after they've slipped.\",\n",
    "    \"C\": \"Write an essay about the difference between choosing and deciding. Thesis: decisions are what you make when the options are legible. Choices are what you make when they aren't. Most of what matters in life is choosing, not deciding.\",\n",
    "}\n",
    "\n",
    "\n",
    "def generate(model, tokenizer, prompt, n=N_SAMPLES):\n",
    "    \"\"\"Generate n samples from the model for a given prompt.\n",
    "\n",
    "    Uses apply_chat_template() to format the prompt with the model's expected\n",
    "    special tokens. This is better than manually wrapping with [INST] tags because:\n",
    "    - The template is version-locked to the tokenizer (won't break across model versions)\n",
    "    - Handles system prompts, BOS/EOS tokens, etc. automatically\n",
    "    \"\"\"\n",
    "    # Format as a single-turn chat: user asks, assistant responds\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    input_text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,  # Adds the assistant turn header so model knows to generate\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "    input_len = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "    outputs_list = []\n",
    "    for i in range(n):\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(**inputs, **GEN_KWARGS)\n",
    "        # Decode only the NEW tokens (skip the prompt)\n",
    "        generated_text = tokenizer.decode(output[0][input_len:], skip_special_tokens=True)\n",
    "        outputs_list.append(generated_text)\n",
    "\n",
    "    return outputs_list\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Generate BASELINE outputs (before fine-tuning)\n",
    "# Save these so we can compare against fine-tuned outputs later.\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "baseline_outputs = {}\n",
    "for name, prompt in canary_prompts.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"BASELINE — Canary {name}\")\n",
    "    print(f\"Prompt: {prompt[:80]}...\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    samples = generate(model, tokenizer, prompt)\n",
    "    baseline_outputs[name] = samples\n",
    "\n",
    "    # Print just the first sample as a preview\n",
    "    print(f\"\\nSample 1 of {N_SAMPLES}:\")\n",
    "    print(samples[0][:500])\n",
    "    print(\"...\" if len(samples[0]) > 500 else \"\")\n",
    "\n",
    "print(f\"\\nBaseline generation complete. {N_SAMPLES} samples per canary saved.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Apply LoRA adapter\n",
    "#\n",
    "# LoRA injects small trainable matrices into specific layers of the frozen model.\n",
    "# We target q_proj and v_proj (the query and value projections in attention).\n",
    "# This is conservative — good for small datasets where you don't want to overfit.\n",
    "# If voice capture is weak, you can expand to [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
    "# or even add \"gate_proj\", \"up_proj\", \"down_proj\" (the MLP layers).\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_R,                        # Rank 16: moderate. Lower = less capacity, higher = more overfitting risk.\n",
    "    lora_alpha=LORA_ALPHA,            # Scaling factor. Effective LR scales with alpha/r.\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Conservative: just attention Q and V.\n",
    "    lora_dropout=0.05,                # Light dropout to regularize on small dataset.\n",
    "    bias=\"none\",                      # Don't train bias terms — not enough data to justify it.\n",
    "    task_type=\"CAUSAL_LM\",            # Tells PEFT this is autoregressive generation.\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# This prints something like \"trainable params: 6,815,744 || all params: 8,030,261,248 || trainable%: 0.0849\"\n",
    "# ~0.1% trainable is expected. If it says 0% something went wrong with prepare_model_for_kbit_training.\n",
    "model.print_trainable_parameters()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Load training data from Google Drive\n",
    "#\n",
    "# Expected format (chat-style JSONL):\n",
    "# {\"messages\": [{\"role\": \"user\", \"content\": \"Write about X...\"}, {\"role\": \"assistant\", \"content\": \"The essay...\"}]}\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "train_path = f\"{DATA_DIR}/llama_train.jsonl\"\n",
    "val_path = f\"{DATA_DIR}/llama_val.jsonl\"\n",
    "\n",
    "train_dataset = load_dataset(\"json\", data_files=train_path, split=\"train\")\n",
    "val_dataset = load_dataset(\"json\", data_files=val_path, split=\"train\")\n",
    "\n",
    "print(f\"Training examples: {len(train_dataset)}\")\n",
    "print(f\"Validation examples: {len(val_dataset)}\")\n",
    "print(f\"\\nFirst training example messages[0] (user prompt):\")\n",
    "print(train_dataset[0][\"messages\"][0][\"content\"][:200])\n",
    "\n",
    "\n",
    "def format_chat(example):\n",
    "    \"\"\"Convert a chat-style example into tokenized input for training.\n",
    "\n",
    "    apply_chat_template converts the messages list into the model's expected format\n",
    "    with all the right special tokens (e.g., <|begin_of_text|>, <|start_header_id|>, etc.).\n",
    "    \"\"\"\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        example[\"messages\"],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False,  # False because the assistant response is already in the messages\n",
    "    )\n",
    "\n",
    "    tokenized = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        max_length=MAX_SEQ_LEN,\n",
    "        padding=False,  # DataCollator handles padding per-batch\n",
    "    )\n",
    "\n",
    "    # labels = input_ids means the model trains on predicting EVERY token, including\n",
    "    # the prompt tokens. This is simpler and fine for small datasets.\n",
    "    # For advanced usage: mask prompt tokens by setting their labels to -100,\n",
    "    # so the model only learns to predict the assistant's response.\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "# Map the formatting function over both datasets\n",
    "train_dataset = train_dataset.map(format_chat, remove_columns=train_dataset.column_names)\n",
    "val_dataset = val_dataset.map(format_chat, remove_columns=val_dataset.column_names)\n",
    "\n",
    "print(f\"\\nTokenized. First example length: {len(train_dataset[0]['input_ids'])} tokens\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Training\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=CHECKPOINT_DIR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    learning_rate=LR,\n",
    "    warmup_steps=10,\n",
    "    logging_steps=1,             # Log every single step. Small dataset = want full-resolution loss curve.\n",
    "    eval_strategy=\"epoch\",       # Evaluate once per epoch (not more — val set is tiny).\n",
    "    save_strategy=\"epoch\",       # Save checkpoint each epoch to Drive for recovery.\n",
    "    save_total_limit=3,          # Keep only the 3 most recent checkpoints to save Drive space.\n",
    "    fp16=True,                   # Mixed precision — faster training, less VRAM.\n",
    "    report_to=\"none\",            # No wandb/tensorboard — we're logging to notebook output.\n",
    "    gradient_accumulation_steps=1,  # Effective batch size = BATCH_SIZE * this. Increase if OOM.\n",
    "    load_best_model_at_end=True, # After training, load the checkpoint with lowest eval loss.\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    ")\n",
    "\n",
    "# DataCollatorForLanguageModeling handles padding within each batch.\n",
    "# mlm=False means causal language modeling (predict next token), not masked LM.\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(f\"  {len(train_dataset)} training examples x {EPOCHS} epochs = {len(train_dataset) * EPOCHS} steps\")\n",
    "print(f\"  Checkpoints saved to: {CHECKPOINT_DIR}\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Generate from FINE-TUNED model\n",
    "# Same canary prompts, same generate() function, same GEN_KWARGS.\n",
    "# The only thing that changed is the model weights (LoRA adapter active).\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "finetuned_outputs = {}\n",
    "for name, prompt in canary_prompts.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"FINE-TUNED — Canary {name}\")\n",
    "    print(f\"Prompt: {prompt[:80]}...\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    samples = generate(model, tokenizer, prompt)\n",
    "    finetuned_outputs[name] = samples\n",
    "\n",
    "    # Print just the first sample as a preview\n",
    "    print(f\"\\nSample 1 of {N_SAMPLES}:\")\n",
    "    print(samples[0][:500])\n",
    "    print(\"...\" if len(samples[0]) > 500 else \"\")\n",
    "\n",
    "print(f\"\\nFine-tuned generation complete. {N_SAMPLES} samples per canary saved.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Side-by-side comparison: baseline vs fine-tuned\n",
    "#\n",
    "# For each canary, print the best sample from baseline and fine-tuned.\n",
    "# \"Best\" here means sample[0] — in practice you'd read all 5 and pick.\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "for name, prompt in canary_prompts.items():\n",
    "    print(f\"\\n{'#'*70}\")\n",
    "    print(f\"  CANARY {name}\")\n",
    "    print(f\"  Prompt: {prompt}\")\n",
    "    print(f\"{'#'*70}\")\n",
    "\n",
    "    print(f\"\\n{'─'*35} BASELINE {'─'*35}\")\n",
    "    print(baseline_outputs[name][0])\n",
    "\n",
    "    print(f\"\\n{'─'*33} FINE-TUNED {'─'*33}\")\n",
    "    print(finetuned_outputs[name][0])\n",
    "\n",
    "    print()  # Blank line between canaries\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"What to look for:\")\n",
    "print(\"  - Does the fine-tuned version sound more like you?\")\n",
    "print(\"  - Is Canary B (novel topic) closer to your voice, or only A (known topic)?\")\n",
    "print(\"  - If only A improved → the model memorized content, not voice.\")\n",
    "print(\"  - If A and B improved but C (long) didn't → learned sentence voice, not structure.\")\n",
    "print(\"=\"*70)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Save LoRA adapter to Google Drive\n",
    "#\n",
    "# This saves ONLY the LoRA weights (~25MB), not the full 8B model.\n",
    "# To use later, you load the base model + this adapter.\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "adapter_path = f\"{DRIVE_BASE}/adapters/llama-voice-v1\"\n",
    "model.save_pretrained(adapter_path)\n",
    "tokenizer.save_pretrained(adapter_path)\n",
    "\n",
    "print(f\"LoRA adapter saved to: {adapter_path}\")\n",
    "print(f\"\\nTo reload this adapter later:\")\n",
    "print(f\"\"\"\\n\\\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# Load base model (same quantization config)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"{MODEL_ID}\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Load LoRA adapter on top\n",
    "model = PeftModel.from_pretrained(base_model, \"{adapter_path}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"{adapter_path}\")\n",
    "\"\"\")"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}