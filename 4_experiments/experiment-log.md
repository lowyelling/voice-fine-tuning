# Experiment Log

## Run 1 — [date]
**Model:** GPT-2-XL / Llama 3.1 8B
**Phase:** Smoke test / Overfitting diagnostic / Scale / Full
**Config:** lr=?, epochs=?, batch_size=1
**LoRA (Llama only):** rank=?, alpha=?
**Data:** [how many pairs, which tiers, any changes from last run]
**Loss:** [final loss, curve shape: smooth drop / plateau / near-zero / oscillating]

### Canary A Output
```

```

### Canary B Output
```

```

### Canary C Output (Llama only)
```

```

### Notes
- What happened:
- What to try next:
- Voice observations (closer / drifting / memorizing):

---

## Run 2 — [date]
**Model:**
**Phase:**
**Config:**
**LoRA (Llama only):**
**Data:**
**Loss:**

### Canary A Output
```

```

### Canary B Output
```

```

### Canary C Output (Llama only)
```

```

### Notes
-
-
-
